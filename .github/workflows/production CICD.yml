name: Deploy Alfresco to Amazon EKS

on:
  push:
    branches:
      - main

env:
  # Credentials for deployment to AWS
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_REGION: ${{ vars.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  # Credentials of repositories
  NEXUS_USER: ${{ secrets.NEXUS_USER }}
  NEXUS_PASSWORD: ${{ secrets.NEXUS_PASSWORD }}
  QUAY_USERNAME: ${{ secrets.QUAY_USERNAME }}
  QUAY_PASSWORD: ${{ secrets.QUAY_PASSWORD }}
  # Docker image repository
  ECR_NAME: ${{ vars.ECR_NAME }}
  TAG: ${{ vars.TAG }}
  TARGETARCH: ${{ vars.TARGETARCH }}
  # Cluster Configuration
  NAMESPACE: ${{ vars.NAMESPACE }}
  EKS_CLUSTER_NAME: ${{ vars.EKS_CLUSTER_NAME }}
  NODE_ROLE_ARN: ${{ secrets.NODE_ROLE_ARN }}
  EKS_SERVICE_ROLE_ARN: ${{ secrets.EKS_SERVICE_ROLE_ARN }}
  EFS_PV_NAME: ${{ vars.EFS_PV_NAME }}
  # DNS Configuration
  DOMAIN: ${{ vars.DOMAIN }}
  CERTIFICATE_ARN: ${{ vars.CERTIFICATE_ARN }}

permissions:
  contents: read

jobs:
  CICD_Alfresco:
    name: Install dependencies
    runs-on: ubuntu-latest
    environment: production
    defaults:
      run:
        shell: bash
#        working-directory: ./CICDAlfresco

    steps:
    - name: Checkout source code
      uses: actions/checkout@v4
    - name: Set up kubectl
      run: |
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        kubectl version --client
    - name: Set up eksctl
      run: |
        curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
        sudo mv /tmp/eksctl /usr/local/bin
        eksctl version
    - name: Set up Helm
      run: |
        curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh
        chmod 700 get_helm.sh
        ./get_helm.sh
        helm version | cut -d + -f 1
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with: 
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ vars.AWS_REGION }}
  # terraform:
    - name: Setup Terraform with specified version on the runner
      uses: hashicorp/setup-terraform@v2
      # with:
        # terraform_version: 1.6.3

    - name: Terraform init
      id: init
      working-directory: ./Terraform
      run: terraform init 

    - name: Terraform validate
      id: validate
      working-directory: ./Terraform
      run: terraform validate

    - name: Terraform plan
      id: plan
      working-directory: ./Terraform
      run: terraform plan -var="aws_region=$AWS_REGION" -var="cluster_name=$EKS_CLUSTER_NAME" -var="cluster_service_role_arn=$EKS_SERVICE_ROLE_ARN" -var="node_role_arn=$NODE_ROLE_ARN" -no-color -input=false -out planfile
      continue-on-error: true

    - name: Terraform plan status
      if: steps.plan.outcome == 'failure'
      run: exit 1

    - name: Terraform Apply
      id: apply
      working-directory: ./Terraform
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: terraform apply -auto-approve -input=false -parallelism=1 planfile

    - name: Get outputs
      uses: dflook/terraform-output@v1
      id: tf-outputs
      with:
        path: ./Terraform

    - name: Print Outputs
      run: |
        echo "VPC_ID=${{steps.tf-outputs.outputs.vpc_id}}"
        echo "EFS_DNS_NAME=${{steps.tf-outputs.outputs.efs_dns_name}}"
        echo "EKS_CLUSTER_ENDPOINT=${{steps.tf-outputs.outputs.eks_cluster_endpoint}}"
        echo "VPC_ID=${{steps.tf-outputs.outputs.vpc_id}}" >> $GITHUB_ENV
        echo "EFS_DNS_NAME=${{steps.tf-outputs.outputs.efs_dns_name}}" >> $GITHUB_ENV
        echo "EKS_CLUSTER_ENDPOINT=${{steps.tf-outputs.outputs.eks_cluster_endpoint}}" >> $GITHUB_ENV

    - name: Get Kube config file
      id: getconfig
      if: steps.apply.outcome == 'success'
      run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
  
    - name: Create namespace and associate IAM OIDC provider
      if: steps.getconfig.outcome == 'success'
      run: |
        if ! kubectl get namespace ${NAMESPACE}; then kubectl create namespace ${NAMESPACE}; fi
        eksctl utils associate-iam-oidc-provider --cluster=$EKS_CLUSTER_NAME --approve
        
    - name: Setup EBS CSI Driver Just to have it if not exits
      run: |
        eksctl create addon --name aws-ebs-csi-driver --cluster $EKS_CLUSTER_NAME --region $AWS_REGION --service-account-role-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/AmazonEKS_EBS_CSI_DriverRole_$EKS_CLUSTER_NAME --force

    - name: Delete EBS CSI Driver in case alrready exists to recreate it
      run: |
        eksctl delete addon --name aws-ebs-csi-driver --cluster $EKS_CLUSTER_NAME --region $AWS_REGION

    - name: Recreate EBS CSI Driver
      run: |
        eksctl create addon --name aws-ebs-csi-driver --cluster $EKS_CLUSTER_NAME --region $AWS_REGION --service-account-role-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/AmazonEKS_EBS_CSI_DriverRole_$EKS_CLUSTER_NAME --force

    - name: Add EFS CSI Driver Helm repo
      run: |
        helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver
        helm repo update

    - name: Install EFS CSI Driver
      working-directory: ./files
      run: |
        envsubst < aws-efs-values.yml | helm upgrade aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver --install --namespace kube-system -f -
    
    - name: Create EFS StorageClass
      working-directory: ./files
      run: |
        if ! kubectl get storageclass 2>/dev/null | grep -q "alfresco-efs-sc-${EFS_PV_NAME}"; then
          envsubst < alf-efs-storage-class.yaml | kubectl create -f -
        fi
      continue-on-error: true
      
    - name: Create EFS Persistent Volume
      working-directory: ./files
      run: |
        if ! kubectl get pv 2>/dev/null | grep -q "alf-content-pv-${EFS_PV_NAME}"; then
          envsubst < alf-content-persistence-volume.yaml | kubectl apply -f -
        fi
      continue-on-error: true
      
    - name: Create EFS Persistent Volume Claim
      working-directory: ./files
      run: |
        if ! kubectl get pvc 2>/dev/null | grep -q "alf-content-pvc-${EFS_PV_NAME}"; then
          envsubst < alf-content-persistence-volume-claim.yaml | kubectl create -f -
        fi
      continue-on-error: true

    - name: Apply External DNS configuration
      working-directory: ./files
      run: |
        kubectl apply -f external-dns.yaml

    - name: Verify External DNS ConfigMap
      run: |
        kubectl get configmap external-dns-config -n kube-system

    - name: Verify External DNS Pods
      run: |
        kubectl get pods -n kube-system -l app=external-dns

    - name: Confirm External DNS application
      run: |
        echo "DNS externo aplicado correctamente."

    - name: Add Ingress NGINX Helm repo
      run: |
        helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
        helm repo update

    - name: Apply Ingress RBAC configuration
      working-directory: ./files
      run: |
        envsubst < ingress-rbac.yaml | kubectl apply -f - -n ${NAMESPACE}

    - name: Install Ingress Controller
      run: |
        helm install acs-ingress-$EKS_CLUSTER_NAME ingress-nginx/ingress-nginx --set controller.scope.enabled=true --set controller.scope.namespace=$NAMESPACE --set rbac.create=true --set controller.config."proxy-body-size"="100m" --set controller.service.targetPorts.https=80 --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-backend-protocol"="http" --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-ports"="https" --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-cert"="${CERTIFICATE_ARN}" --set controller.service.annotations."external-dns\.alpha\.kubernetes\.io/hostname"="acs.${DOMAIN}" --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-negotiation-policy"="ELBSecurityPolicy-TLS-1-2-2017-01" --set controller.publishService.enabled=true --set controller.ingressClassResource.name="$NAMESPACE-nginx" --set controller.ingressClassByName=true --atomic --namespace $NAMESPACE

    - name: Verify Ingress Controller pods
      run: kubectl get pods -n ${NAMESPACE}

    - name: Verify kube-system pods
      run: kubectl get pods -n kube-system
    - name: Create Docker registry secret
      run: |
        kubectl create secret docker-registry quay-registry-secret --docker-server=quay.io --docker-username=$QUAY_USERNAME --docker-password=$QUAY_PASSWORD -n ${NAMESPACE}

    - name: Install Alfresco Content Services
      run: |
        helm install acs $CODEBUILD_SRC_DIR/CICDAlfresco/alfresco-content-services \
          --set externalPort="443" \
          --set externalProtocol="https" \
          --set externalHost="acs.${DOMAIN}" \
          --set persistence.enabled=true \
          --set persistence.storageClass.enabled=true \
          --set persistence.storageClass.name="nfs-client" \
          --set alfresco-repository.persistence.existingClaim="alf-content-pvc" \
          --set alfresco-repository.persistence.enabled=true \
          --set global.alfrescoRegistryPullSecrets=quay-registry-secret \
          --set alfresco-sync-service.enabled=false \
          --set postgresql-sync.enabled=false \
          --set alfresco-transform-service.transformrouter.replicaCount="1" \
          --set alfresco-transform-service.pdfrenderer.replicaCount="1" \
          --set alfresco-transform-service.imagemagick.replicaCount="1" \
          --set alfresco-transform-service.libreoffice.replicaCount="1" \
          --set alfresco-transform-service.tika.replicaCount="1" \
          --set alfresco-transform-service.transformmisc.replicaCount="1" \
          --set alfresco-transform-service.transformrouter.resources.limits.memory="2Gi" \
          --set alfresco-transform-service.pdfrenderer.resources.limits.memory="2Gi" \
          --set alfresco-transform-service.imagemagick.resources.limits.memory="2Gi" \
          --set alfresco-transform-service.libreoffice.resources.limits.memory="2Gi" \
          --set alfresco-transform-service.tika.resources.limits.memory="2Gi" \
          --set alfresco-transform-service.transformmisc.resources.limits.memory="2Gi" \
          --set alfresco-transform-service.transformrouter.resources.limits.cpu="250m" \
          --set alfresco-transform-service.pdfrenderer.resources.limits.cpu="250m" \
          --set alfresco-transform-service.imagemagick.resources.limits.cpu="250m" \
          --set alfresco-transform-service.libreoffice.resources.limits.cpu="250m" \
          --set alfresco-transform-service.tika.resources.limits.cpu="250m" \
          --set alfresco-transform-service.transformmisc.resources.limits.cpu="250m" \
          --set alfresco-transform-service.filestore.resources.limits.cpu="250m" \
          --set postgresql.primary.resources.requests.cpu="250m" \
          --set postgresql.primary.resources.limits.cpu="500m" \
          --set postgresql.primary.resources.limits.memory="6Gi" \
          --set alfresco-share.resources.limits.cpu="250m" \
          --set alfresco-search-enterprise.resources.requests.cpu="250m" \
          --set alfresco-search-enterprise.resources.limits.cpu="250m" \
          --set alfresco-repository.resources.requests.cpu="500m" \
          --set alfresco-repository.resources.limits.cpu="500m" \
          --set alfresco-repository.readinessProbe.periodSeconds="200" \
          --set alfresco-repository.livenessProbe.periodSeconds="200" \
          --set alfresco-repository.startupProbe.periodSeconds="200" \
          --set alfresco-transform-service.pdfrenderer.livenessProbe.periodSeconds="200" \
          --set alfresco-transform-service.pdfrenderer.readinessProbe.periodSeconds="200" \
          --set alfresco-transform-service.imagemagick.livenessProbe.periodSeconds="200" \
          --set alfresco-transform-service.imagemagick.readinessProbe.periodSeconds="200" \
          --set alfresco-transform-service.tika.livenessProbe.periodSeconds="200" \
          --set alfresco-transform-service.tika.readinessProbe.periodSeconds="200" \
          --set alfresco-transform-service.libreoffice.livenessProbe.periodSeconds="200" \
          --set alfresco-transform-service.libreoffice.readinessProbe.periodSeconds="200" \
          --set alfresco-transform-service.transformmisc.livenessProbe.periodSeconds="200" \
          --set alfresco-transform-service.transformmisc.readinessProbe.periodSeconds="200" \
          --set alfresco-search-enterprise.reindexing.enabled=false \
          --timeout 20m0s \
          --namespace=$NAMESPACE

    - name: Verify Alfresco Content Services pods
      run: kubectl get pods -n ${NAMESPACE}

    - name: Output deployment details
      run: |
        echo "DOMAIN=$DOMAIN"
        echo "NAMESPACE=$NAMESPACE"
        echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME"
        echo "Despliegue finalizado"

#  bake:
#    name: Deploy
#    runs-on: ubuntu-latest
#
#    steps:
#    - name: Checkout
#      uses: actions/checkout@v2
#
#    - name: Authenticate with Nexus
#      run: |
#        chmod +x ./Scripts/setup-netrc.sh
#        ./Scripts/setup-netrc.sh
#        echo "ValidaciÃ³n del archivo .netrc completada"









