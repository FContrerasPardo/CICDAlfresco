name: CD - Deploy Alfresco to Amazon EKS

on:
  push:
    branches:
      - main

env:
  # Credentials for deployment to AWS
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_REGION: ${{ vars.AWS_REGION }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
  # Credentials of repositories
  NEXUS_USER: ${{ secrets.NEXUS_USER }}
  NEXUS_PASSWORD: ${{ secrets.NEXUS_PASSWORD }}
  QUAY_USERNAME: ${{ secrets.QUAY_USERNAME }}
  QUAY_PASSWORD: ${{ secrets.QUAY_PASSWORD }}
  # Docker image repository
  ECR_NAME: ${{ vars.ECR_NAME }}
  TAG: ${{ vars.TAG }}
  TARGETARCH: ${{ vars.TARGETARCH }}
  # Cluster Configuration
  NAMESPACE: ${{ vars.NAMESPACE }}
  EKS_CLUSTER_NAME: ${{ vars.EKS_CLUSTER_NAME }}
  NODE_ROLE_ARN: ${{ secrets.NODE_ROLE_ARN }}
  EKS_SERVICE_ROLE_ARN: ${{ secrets.EKS_SERVICE_ROLE_ARN }}
  EFS_PV_NAME: ${{ vars.EFS_PV_NAME }}
  # DNS Configuration
  DOMAIN: ${{ vars.DOMAIN }}
  CERTIFICATE_ARN: ${{ vars.CERTIFICATE_ARN }}

permissions:
  contents: read

jobs:
  CICD_Alfresco:
    name: Install dependencies
    runs-on: ubuntu-latest
    environment: production
    defaults:
      run:
        shell: bash
#        working-directory: ./CICDAlfresco

    steps:
    - name: Checkout source code
      uses: actions/checkout@v4
    - name: Set up kubectl
      run: |
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        kubectl version --client
    - name: Set up eksctl
      run: |
        curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
        sudo mv /tmp/eksctl /usr/local/bin
        eksctl version
    - name: Set up Helm
      run: |
        curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh
        chmod 700 get_helm.sh
        ./get_helm.sh
        helm version | cut -d + -f 1
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with: 
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ vars.AWS_REGION }}
  # terraform:
    - name: Setup Terraform with specified version on the runner
      uses: hashicorp/setup-terraform@v2
      # with:
        # terraform_version: 1.6.3

    - name: Terraform init
      id: init
      working-directory: ./Terraform
      run: terraform init 

    - name: Terraform validate
      id: validate
      working-directory: ./Terraform
      run: terraform validate

    - name: Terraform plan
      id: plan
      working-directory: ./Terraform
      run: terraform plan -var="aws_region=$AWS_REGION" -var="cluster_name=$EKS_CLUSTER_NAME" -var="cluster_service_role_arn=$EKS_SERVICE_ROLE_ARN" -var="node_role_arn=$NODE_ROLE_ARN" -no-color -input=false -out planfile
      continue-on-error: true

    - name: Terraform plan status
      if: steps.plan.outcome == 'failure'
      run: exit 1

    - name: Terraform Apply
      id: apply
      working-directory: ./Terraform
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: terraform apply -auto-approve planfile

    - name: Get outputs
      uses: dflook/terraform-output@v1
      id: tf-outputs
      with:
        path: ./Terraform

    - name: Print Outputs
      run: |
        echo "VPC_ID=${{steps.tf-outputs.outputs.vpc_id}}"
        echo "EFS_ID=${{steps.tf-outputs.outputs.efs_id}}"
        echo "EFS_DNS_NAME=${{steps.tf-outputs.outputs.efs_dns_name}}"
        echo "EKS_CLUSTER_ENDPOINT=${{steps.tf-outputs.outputs.eks_cluster_endpoint}}"
        echo "EKS_NODE_GROUP_NAME=${{steps.tf-outputs.outputs.aws_eks_node_group}}"
        echo "VPC_ID=${{steps.tf-outputs.outputs.vpc_id}}" >> $GITHUB_ENV
        echo "EFS_ID=${{steps.tf-outputs.outputs.efs_id}}" >> $GITHUB_ENV
        echo "EFS_DNS_NAME=${{steps.tf-outputs.outputs.efs_dns_name}}" >> $GITHUB_ENV
        echo "EKS_CLUSTER_ENDPOINT=${{steps.tf-outputs.outputs.eks_cluster_endpoint}}" >> $GITHUB_ENV
        echo "EKS_NODE_GROUP_NAME=${{steps.tf-outputs.outputs.aws_eks_node_group}}" >> $GITHUB_ENV
  #Storage Configuration
    - name: Get Kube config file
      id: getconfig
      if: steps.apply.outcome == 'success'
      run: aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
  
    - name: Create namespace
      if: steps.getconfig.outcome == 'success'
      run: |
        kubectl get namespace ${NAMESPACE} || \
        kubectl create namespace ${NAMESPACE}
        
    - name: associate IAM OIDC provider
      run: |
        eksctl utils associate-iam-oidc-provider --cluster=$EKS_CLUSTER_NAME --region $AWS_REGION  --approve
    - name: Create IAM role and service account for EBS CSI Driver
      run: |
        eksctl create iamserviceaccount \
          --cluster $EKS_CLUSTER_NAME \
          --region $AWS_REGION \
          --name ebs-csi-controller-sa \
          --namespace kube-system \
          --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
          --override-existing-serviceaccounts \
          --approve
#    - name: Create IAM role for EBS CSI Driver
#      run: |
#       eksctl create iamserviceaccount \
#       --name ebs-csi-controller-sa \
#       --namespace kube-system \
#       --cluster $EKS_CLUSTER_NAME \
#       --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
#       --approve \
#       --role-only \
#       --role-name AmazonEKS_EBS_CSI_DriverRole
    - name: Get EBS CSI Role ARN from ServiceAccount
      id: get-ebs-csi-role
      run: |
        # Describimos el SA para extraer la anotación con el ARN del rol
        SA_INFO=$(kubectl describe sa ebs-csi-controller-sa -n kube-system)

        # Extraemos la línea relevante y eliminamos cualquier texto adicional antes del ARN
        ROLE_ARN=$(echo "$SA_INFO" | grep 'eks.amazonaws.com/role-arn:' | sed -E 's/.*eks.amazonaws.com\/role-arn: //')

        if [ -z "$ROLE_ARN" ]; then
          echo "ERROR: No se encontró la anotación 'eks.amazonaws.com/role-arn' en el ServiceAccount ebs-csi-controller-sa."
          exit 1
        fi

        echo "Encontrado ROLE_ARN=$ROLE_ARN"

        # Guardamos en el entorno de GitHub Actions
        echo "ROLE_ARN=$ROLE_ARN" >> $GITHUB_ENV
    - name: Create EBS CSI Add-on
      run: |
        echo "Usaremos el ARN: ${{ env.ROLE_ARN }}"
        eksctl create addon \
          --name aws-ebs-csi-driver \
          --cluster $EKS_CLUSTER_NAME \
          --region $AWS_REGION \
          --service-account-role-arn "${{ env.ROLE_ARN }}" \
          --force        
#    - name: Setup EBS CSI Driver Just to have it if not exits
#      run: |
#        eksctl get addon --name aws-ebs-csi-driver --cluster $EKS_CLUSTER_NAME --region $AWS_REGION || \
#        eksctl create addon --name aws-ebs-csi-driver --cluster $EKS_CLUSTER_NAME --region $AWS_REGION --service-account-role-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/AmazonEKS_EBS_CSI_DriverRole --force

    - name: Setup Default Driver
      run: |
        kubectl patch storageclass gp2 \
          -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

#     Si se desea almacenar todo en EFS por defecto 
#      # primero desmarcas gp2 si estuviera default
#      kubectl patch storageclass gp2 \
#        -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}' \
#        || true
##     # luego marcas alfresco-efs-sc como default
#      kubectl patch storageclass alfresco-efs-sc \
#        -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

##     Si se desea almacenar todo en nfs-client por defecto 
#      1) Quitar la anotación de default al SC que hoy está como default (en tu caso, alfresco-efs-sc)
#      kubectl patch storageclass alfresco-efs-sc \
#        -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
#            
#      # 2) Marcar como default el SC “nfs-client”
#      kubectl patch storageclass nfs-client \
#        -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

    - name: Add EFS CSI Driver Helm repo
      run: |
        helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver
        helm repo update

    - name: Install EFS CSI Driver
      working-directory: ./files
      run: |
        envsubst < aws-efs-values.yml | helm upgrade aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver --install --namespace kube-system -f -
        kubectl get pod -n kube-system -l "app.kubernetes.io/name=aws-efs-csi-driver,app.kubernetes.io/instance=aws-efs-csi-driver"

    - name: Create EFS StorageClass
      working-directory: ./files
      run: |
        if ! kubectl get storageclass -n ${NAMESPACE} 2>/dev/null | grep -q "alfresco-efs-sc"; then
          envsubst < alf-efs-storage-class.yaml | kubectl create -f -
        fi
      continue-on-error: true
      
    - name: Create EFS Persistent Volume
      working-directory: ./files
      run: |
          kubectl get pv -n ${NAMESPACE} | grep -q "alf-content-pv" || \
          envsubst < alf-content-persistence-volume.yaml | kubectl apply -f -
      continue-on-error: true
      
    - name: Create EFS Persistent Volume Claim
      working-directory: ./files
      run: |
        kubectl get pvc -n ${NAMESPACE} | grep -q "alf-content-pvc" || \
        envsubst < alf-content-persistence-volume-claim.yaml | kubectl create -f -
      continue-on-error: true

    # External DNS Configuration and Load Balancer
    - name: Apply External DNS configuration
      working-directory: ./files
      run: |
        kubectl apply -f external-dns.yaml
    - name: Create External DNS Service Account
      run: | 
        eksctl create iamserviceaccount \
        --cluster $EKS_CLUSTER_NAME \
        --namespace kube-system \
        --name external-dns \
        --attach-policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess \
        --approve \
        --override-existing-serviceaccounts

    - name: Verify External DNS
      run: |
        kubectl get configmap external-dns-config -n kube-system
        kubectl describe configmap external-dns-config -n kube-system
        kubectl get pods -n kube-system -l app=external-dns

# esto no es necesario por que en terraform se esta asignando el rol con todo ya definido
#    - name: Configure IAM Role for NodeGroup
#        
#      run: |
#        # 1. Lista los Node Groups del clúster EKS
#        NODEGROUP=$(aws eks list-nodegroups --cluster-name $EKS_CLUSTER_NAME --query "nodegroups[0]" --output text)
#        
#        if [ "$NODEGROUP" == "None" ]; then
#          echo "No se encontró un grupo de nodos para el clúster $EKS_CLUSTER_NAME"
#          exit 1
#        fi
#    
#        echo "NodeGroup encontrado: $NODEGROUP"
#    
#        # 2. Encuentra el nombre del rol asociado al NodeGroup
#        NODE_ROLE=$(aws eks describe-nodegroup --cluster-name $EKS_CLUSTER_NAME --nodegroup-name $NODEGROUP --query "nodegroup.nodeRole" --output text)
#    
#        if [ -z "$NODE_ROLE" ]; then
#          echo "No se encontró un rol asociado al NodeGroup $NODEGROUP"
#          exit 1
#        fi
#    
#        echo "Rol de IAM encontrado: $NODE_ROLE"
#    
#        # 3. Adjunta la política administrada AmazonRoute53FullAccess al rol
#        aws iam attach-role-policy --role-name $(basename $NODE_ROLE) --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess
#    
#        echo "Política AmazonRoute53FullAccess adjuntada al rol $NODE_ROLE"


    - name: Add Ingress NGINX Helm repo
      run: |
        helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
        helm repo update

    # Install Ingress Controller Generico
#    - name: Install Ingress Controller
#      working-directory: ./files
#      run: |
#        helm upgrade --install ingress-nginx ingress-nginx \
#        --repo https://kubernetes.github.io/ingress-nginx \
#        --namespace ingress-nginx --create-namespace \
#        --version 4.7.2 \
#        --set controller.allowSnippetAnnotations=true
#
#        helm delete ingress-nginx -n ingress-nginx


#   El término "Ingress RBAC" se refiere a la configuración de permisos (roles y vinculaciones de roles) necesarios para que el controlador de Ingress (como el Ingress NGINX) pueda acceder y operar sobre los recursos de Kubernetes (servicios, endpoints, pods, etc.) que utiliza para enrutar el tráfico. Esto se define mediante objetos de tipo ClusterRole y ClusterRoleBinding en Kubernetes, y permite que el controlador pueda listar, ver y monitorear las reglas de Ingress, así como otros recursos relacionados.
    - name: Apply Ingress RBAC configuration
      working-directory: ./files
      run: |
        envsubst < ingress-rbac.yaml | kubectl apply -f - -n ${NAMESPACE}

# Este comando despliega el controlador de Ingress NGINX en tu clúster de Kubernetes y configura automáticamente un balanceador de carga en AWS (con certificado SSL en el puerto 443) que enruta las peticiones hacia el Ingress Controller. Así, cualquier solicitud que llegue al dominio configurado (por ejemplo, acs.tfmfc.com) se redirige internamente a los pods de Alfresco o de otras aplicaciones, permitiendo exponerlos de forma segura y unificada.
    - name: Install Ingress Controller
      run: |
        helm upgrade acs-ingress ingress-nginx/ingress-nginx --install\
        --set controller.scope.enabled=true \
        --set controller.scope.namespace=$NAMESPACE \
        --set rbac.create=true \
        --set controller.config."proxy-body-size"="100m" \
        --set controller.service.targetPorts.https=80 \
        --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-backend-protocol"="http" \
        --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-ports"="https" \
        --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-cert"="${CERTIFICATE_ARN}" \
        --set controller.service.annotations."external-dns\.alpha\.kubernetes\.io/hostname"="acs.${DOMAIN}" \
        --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-ssl-negotiation-policy"="ELBSecurityPolicy-TLS-1-2-2017-01" \
        --set controller.publishService.enabled=true \
        --set controller.ingressClassResource.name="nginx" \
        --set controller.ingressClassByName=true \
        --atomic --namespace $NAMESPACE
    - name: Verify Ingress Controller pods
      run: | 
        kubectl get pods -n ${NAMESPACE}
        kubectl get pods -n kube-system

    - name: Create Docker Registry Secret if not exists
      run: |
        kubectl get secret quay-registry-secret -n ${NAMESPACE} || \
        kubectl create secret docker-registry quay-registry-secret \
          --docker-server=quay.io \
          --docker-username=$QUAY_USERNAME \
          --docker-password=$QUAY_PASSWORD \
          -n ${NAMESPACE}

    - name: Install Alfresco Content Services
      id : helm_install
      run: |
        helm upgrade acs ./alfresco-content-services --install \
        --set externalPort="443" \
        --set externalProtocol="https" \
        --set externalHost="acs.${DOMAIN}" \
        --set global.known_urls=https://acs.${DOMAIN} \
        --set persistence.enabled=true \
        --set persistence.storageClass.enabled=true \
        --set persistence.storageClass.name="nfs-client" \
        --set alfresco-repository.persistence.existingClaim="alf-content-pvc" \
        --set alfresco-repository.persistence.enabled=true \
        --set alfresco-repository.image.tag="23.4.1" \
        --set postgresql.volumePermissions.enabled=true \
        --set global.alfrescoRegistryPullSecrets=quay-registry-secret \
        --set alfresco-sync-service.enabled=false \
        --set postgresql-sync.enabled=false \
        --set alfresco-transform-service.transformrouter.replicaCount="1" \
        --set alfresco-transform-service.pdfrenderer.replicaCount="1" \
        --set alfresco-transform-service.imagemagick.replicaCount="1" \
        --set alfresco-transform-service.libreoffice.replicaCount="1" \
        --set alfresco-transform-service.tika.replicaCount="1" \
        --set alfresco-transform-service.transformmisc.replicaCount="1" \
        --set alfresco-transform-service.transformrouter.resources.limits.memory="2Gi" \
        --set alfresco-transform-service.pdfrenderer.resources.limits.memory="2Gi" \
        --set alfresco-transform-service.imagemagick.resources.limits.memory="2Gi" \
        --set alfresco-transform-service.libreoffice.resources.limits.memory="2Gi" \
        --set alfresco-transform-service.tika.resources.limits.memory="2Gi" \
        --set alfresco-transform-service.transformmisc.resources.limits.memory="2Gi" \
        --set alfresco-transform-service.transformrouter.resources.limits.cpu="250m" \
        --set alfresco-transform-service.pdfrenderer.resources.limits.cpu="250m" \
        --set alfresco-transform-service.imagemagick.resources.limits.cpu="250m" \
        --set alfresco-transform-service.libreoffice.resources.limits.cpu="250m" \
        --set alfresco-transform-service.tika.resources.limits.cpu="250m" \
        --set alfresco-transform-service.transformmisc.resources.limits.cpu="250m" \
        --set alfresco-transform-service.filestore.resources.limits.cpu="250m" \
        --set postgresql.primary.resources.requests.cpu="250m" \
        --set postgresql.primary.resources.limits.cpu="500m" \
        --set postgresql.primary.resources.limits.memory="6Gi" \
        --set alfresco-share.resources.limits.cpu="250m" \
        --set alfresco-search-enterprise.resources.requests.cpu="250m" \
        --set alfresco-search-enterprise.resources.limits.cpu="250m" \
        --set alfresco-repository.resources.requests.cpu="500m" \
        --set alfresco-repository.resources.limits.cpu="500m" \
        --set alfresco-repository.readinessProbe.periodSeconds="200" \
        --set alfresco-repository.livenessProbe.periodSeconds="200" \
        --set alfresco-repository.startupProbe.periodSeconds="200" \
        --set alfresco-transform-service.pdfrenderer.livenessProbe.periodSeconds="200" \
        --set alfresco-transform-service.pdfrenderer.readinessProbe.periodSeconds="200" \
        --set alfresco-transform-service.imagemagick.livenessProbe.periodSeconds="200" \
        --set alfresco-transform-service.imagemagick.readinessProbe.periodSeconds="200" \
        --set alfresco-transform-service.tika.livenessProbe.periodSeconds="200" \
        --set alfresco-transform-service.tika.readinessProbe.periodSeconds="200" \
        --set alfresco-transform-service.libreoffice.livenessProbe.periodSeconds="200" \
        --set alfresco-transform-service.libreoffice.readinessProbe.periodSeconds="200" \
        --set alfresco-transform-service.transformmisc.livenessProbe.periodSeconds="200" \
        --set alfresco-transform-service.transformmisc.readinessProbe.periodSeconds="200" \
        --set alfresco-share.livenessProbe.periodSeconds="200" \
        --set alfresco-share.readinessProbe.periodSeconds="200" \
        --set alfresco-search-enterprise.reindexing.enabled=false \
        --timeout 20m0s \
        --namespace=$NAMESPACE

    - name: Watch Helm deployment
      env:
        HELM_INSTALL_TIMEOUT: 10m
      run: |
        kubectl get pods --watch -n $NAMESPACE &
        KWPID=$!
        kubectl wait --timeout=5m --all=true --for=condition=Ready pods -n $NAMESPACE
        kill $KWPID
      continue-on-error: true

    - name: Debug cluster status after install
      if: always() && steps.helm_install.outcome != 'skipped'
      run: |
        helm ls --all-namespaces --all
        kubectl get all --all-namespaces
        kubectl describe pod

    - name: Verify Alfresco Content Services pods
      run: kubectl get pods -n ${NAMESPACE}

    - name: Output deployment details
      run: |
        echo "DOMAIN=$DOMAIN"
        echo "NAMESPACE=$NAMESPACE"
        echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME"
        echo "Despliegue finalizado"

#  bake:
#    name: Deploy
#    runs-on: ubuntu-latest
#
#    steps:
#    - name: Checkout
#      uses: actions/checkout@v2
#
#    - name: Authenticate with Nexus
#      run: |
#        chmod +x ./Scripts/setup-netrc.sh
#        ./Scripts/setup-netrc.sh
#        echo "Validación del archivo .netrc completada"









